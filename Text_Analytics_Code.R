# Text-Analytics


library(pdftools)
library(tm)
library(wordcloud)
library(dplyr)
library(topicmodels)
library(tidytext)
library(ggplot2)



book = pdf_text('E:/Machine Learning/My practice/text_books/ISLR Seventh Printing.pdf')
reviews = read.csv('E:/Machine Learning/My practic/Kaggle_AV projects/class datasets/datasets-master/amazon_reviews_11.csv')
hotstar = read.csv('E:/Machine Learning/My practic/Kaggle_AV projects/class datasets/datasets-master/hotstar.allreviews_Sentiments.csv')
regex_func = function(x){ return (gsub('[^a-z ]', '', x)) }

common_stop_words = stopwords()
custom_stop_words = c('set', 'can', 'get', 'will',
                      'using')
all_stop_words = append(common_stop_words, custom_stop_words)
docs = VCorpus(VectorSource(as.character(reviews$reviewText)))
docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, content_transformer(regex_func))
docs = tm_map(docs, stripWhitespace)
docs = tm_map(docs, removeWords, all_stop_words)
dtm = DocumentTermMatrix(docs)
df_dtm = as.data.frame(as.matrix(dtm))
dim(df_dtm)
inspect(docs[[1]])

x = colSums(df_dtm)
words_freq = data.frame(words=labels(x),
                        freq=x)
#words_freq %>% arrange(-freq) %>% head(50)
wordcloud(words_freq$words,
          words_freq$freq,
          max.words = 100,
          scale=c(3,0.1))

#Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre-9.0.4')
library(rJava)
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
dtm_bigram = DocumentTermMatrix(docs, control=list(tokenize=BigramTokenizer))
df_dtm_bigram = as.data.frame(as.matrix(dtm_bigram))

x = colSums(df_dtm_bigram)
bigrams_words_freq = data.frame(words=labels(x),
                                freq=x)
# bigrams_words_freq %>% arrange(-freq) %>% head(10)
wordcloud(bigrams_words_freq$words,
          bigrams_words_freq$freq,
          max.words = 100,

          
negative_words = c('bad', 'waste', 'wastage', 'pathetic',
                   'terrible', 'fraud', 'attrocious', 
                   'horrible', 'bullshit', 'unsatisfied',
                   'ridiculous', 'absurd')

bigrams = colnames(df_dtm_bigram)

negative_bigrams = c()
for (bigram in bigrams){
  words = unlist(strsplit(bigram, ' '))
  if (length(intersect(negative_words, words))>0){
    negative_bigrams = append(negative_bigrams, bigram)
  }
}

head(sort(colSums(df_dtm_bigram[, negative_bigrams]), decreasing = T), 5)



### Word similarity

library(lsa)
word1_vec = df_dtm[, 'kindle']
word2_vec = df_dtm[, 'book']
lsa::cosine(word1_vec, word2_vec)

words_similar = function(word, df_dtm, n){
  word2_compare = c()
  word1_vec = df_dtm[, word]
  words_cs = c()
  for (w in colnames(df_dtm)){
    if(word != w){
      word2_vec = df_dtm[, w]
      cs = cosine(word1_vec, word2_vec)[1]
      word2_compare = append(word2_compare, w)
      words_cs = append(words_cs, cs)
    }
  }
  result = data.frame(word=word2_compare, cosine=words_cs)
  result = result %>% arrange(-cosine) %>% head(n)
  return (result)
  
}
words_similar('kindle', df_dtm, 10)





### Document Similarity

documents_similar = function(doc_row, df_dtm){
  document_rows = c()
  cosine_values = c()
  doc1_vec = as.numeric(df_dtm[doc_row, ])
  for (row in seq(1, nrow(df_dtm))){
    if (row != doc_row){
      doc2_vec = as.numeric(df_dtm[row, ])
      cs = lsa::cosine(doc1_vec, doc2_vec)[1]
      document_rows = append(document_rows, row)
      cosine_values = append(cosine_values, cs)
    }
  }
  result = data.frame("doc_no"=document_rows, "cosine"=cosine_values)
  result = result %>% arrange(-cosine) %>% head(5)
  return (result)
}
documents_similar(10, df_dtm)



### Sentiment Analysis

#### Unsuperised methods

library(RSentiment)
calculate_score(c('Teaching is pathetic',
                  'I do not like data science',
                  'But gives good package and its an illusion',
                  'I do not like R codding'))


sentiments = calculate_score(head(as.character(reviews$reviewText)))
hist(sentiments)



### Supervised Analysis

#View(hotstar %>% select(Reviews, Sentiment_Manual))
df_dtm$sentiment = hotstar$Sentiment_Manual
train = df_dtm[1:3000,]
test = df_dtm[3001:nrow(df_dtm),]
library(randomForest)
library(rpart)

model = rpart(sentiment~., data=train)
test$sentiment_pred = predict(model, test, type='class')

sum(test$sentiment == test$sentiment_pred) / nrow(test) * 100


### High dimensional reduction
```{r}
dtm_nonsparse = removeSparseTerms(dtm, sparse = 0.98)
df_dtm_nonsparse = as.data.frame(as.matrix(dtm_nonsparse))
dim(df_dtm_nonsparse)

dtm_nonsparse = removeSparseTerms(dtm, sparse=0.95)
dtm_nonsparse = dtm_nonsparse[rowSums(as.matrix(dtm_nonsparse))>0,]
lda.out = LDA(dtm_nonsparse, 4, method='Gibbs')

word2topic = tidy(lda.out, matrix="beta")
doc2topic = tidy(lda.out, matrix="gamma")
View(word2topic)
View(doc2topic)

common_stop_words = stopwords()
custom_stop_words = c('set', 'can', 'get', 'will',
                      'using', 'use', 'good', 'well', 'bought', 'great',
                      'just', 'one', 'time', 'still', 'now')
all_stop_words = append(common_stop_words, custom_stop_words)
docs = VCorpus(VectorSource(as.character(reviews$reviewText)))
docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, content_transformer(regex_func))
docs = tm_map(docs, stripWhitespace)
docs = tm_map(docs, removeWords, all_stop_words)
dtm = DocumentTermMatrix(docs)
df_dtm = as.data.frame(as.matrix(dtm))

dtm_nonsparse = removeSparseTerms(dtm, sparse=0.95)
dtm_nonsparse = dtm_nonsparse[rowSums(as.matrix(dtm_nonsparse))>0,]
lda.out = LDA(dtm_nonsparse, 4, method='Gibbs')

word2topic = tidy(lda.out, matrix="beta")
doc2topic = tidy(lda.out, matrix="gamma")


word2topic %>% group_by(topic) %>% 
  arrange(topic, -beta) %>% top_n(5) %>% 
  ggplot(aes(x=reorder(term, beta), y=beta)) + geom_bar(stat='identity') + 
  facet_wrap(~topic, scales='free') + coord_flip()

